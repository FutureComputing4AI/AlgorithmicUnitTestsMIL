{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This code was written using some libraries that require Pytorch versions pre-2.0, and some other tools. Installing these first, and was written to run in Google Colab."
      ],
      "metadata": {
        "id": "TPkpmuZQ9qqk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is _research code_ and not meant for production use. No warranty or support will be provided."
      ],
      "metadata": {
        "id": "648TtfIABlPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.13.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDVHVueetWl8",
        "outputId": "01f3af58-0990-4796-8a7f-0fb8893bd2df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.13.1\n",
            "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1) (4.7.1)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.1)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.1)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.1)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (0.41.0)\n",
            "Installing collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1+cu118\n",
            "    Uninstalling torch-2.0.1+cu118:\n",
            "      Successfully uninstalled torch-2.0.1+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Njryd1MwvjUm",
        "outputId": "f9a876f8-c81c-4ecb-99d7-430867eadde3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining misvm from git+https://github.com/garydoranjr/misvm.git#egg=misvm\n",
            "  Updating ./src/misvm clone\n",
            "  Running command git fetch -q --tags\n",
            "  Running command git reset --hard -q b2118fe04d98c00436bdf8a0e4bbfb6082c5751c\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from misvm) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from misvm) (1.10.1)\n",
            "Requirement already satisfied: cvxopt in /usr/local/lib/python3.10/dist-packages (from misvm) (1.3.1)\n",
            "Installing collected packages: misvm\n",
            "  Attempting uninstall: misvm\n",
            "    Found existing installation: misvm 1.0\n",
            "    Uninstalling misvm-1.0:\n",
            "      Successfully uninstalled misvm-1.0\n",
            "  Running setup.py develop for misvm\n",
            "Successfully installed misvm-1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -e git+https://github.com/garydoranjr/misvm.git#egg=misvm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WphusGRPdxYT"
      },
      "outputs": [],
      "source": [
        "import misvm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkkH-Op3BIrR",
        "outputId": "74a0a4a8-737a-40bb-83dc-7693b99389df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/ml-jku/hopfield-layers\n",
            "  Cloning https://github.com/ml-jku/hopfield-layers to /tmp/pip-req-build-ln3tpo9t\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/ml-jku/hopfield-layers /tmp/pip-req-build-ln3tpo9t\n",
            "  Resolved https://github.com/ml-jku/hopfield-layers to commit f56f929c95b77a070ae675ea4f56b6d54d36e730\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from hopfield-layers==1.0.2) (1.13.1)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from hopfield-layers==1.0.2) (1.23.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->hopfield-layers==1.0.2) (4.7.1)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->hopfield-layers==1.0.2) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->hopfield-layers==1.0.2) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->hopfield-layers==1.0.2) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->hopfield-layers==1.0.2) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.5.0->hopfield-layers==1.0.2) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.5.0->hopfield-layers==1.0.2) (0.41.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/ml-jku/hopfield-layers #This needs older Python :("
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQGN5FdZyjeB",
        "outputId": "a5fef98b-4625-483d-f341-193cdd33987a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-1.13.0+cpu.html\n",
            "Requirement already satisfied: pyg-lib in /usr/local/lib/python3.10/dist-packages (0.2.0+pt113cpu)\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.10/dist-packages (2.1.1+pt113cpu)\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.10/dist-packages (0.6.17+pt113cpu)\n",
            "Requirement already satisfied: torch-cluster in /usr/local/lib/python3.10/dist-packages (1.6.1+pt113cpu)\n",
            "Requirement already satisfied: torch-spline-conv in /usr/local/lib/python3.10/dist-packages (1.2.2+pt113cpu)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.10.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.23.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2023.7.22)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyg-lib torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.13.0+cpu.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49qDcbo0grmE"
      },
      "outputs": [],
      "source": [
        "from scipy.io import loadmat\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8wnQg2SgrmG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import numpy as np\n",
        "import string\n",
        "import pathlib\n",
        "import gzip\n",
        "from numba import jit\n",
        "\n",
        "from tqdm.autonotebook import tqdm\n",
        "\n",
        "import random\n",
        "import heapq\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGwnLEaXgrmG"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import fbeta_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYA_9SCgyzlX"
      },
      "outputs": [],
      "source": [
        "from torch import Tensor\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3wvXwV7BBr7"
      },
      "outputs": [],
      "source": [
        "from hflayers import Hopfield\n",
        "from hflayers import HopfieldLayer\n",
        "from hflayers import HopfieldPooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGshgHnygrmL"
      },
      "outputs": [],
      "source": [
        "# Algorithm 1 Single-Concept Standard-MIL\n",
        "def falseNegativeMIL(train_size, test_size, min_size=None, d=16):\n",
        "    labels_train = []\n",
        "    bags_train = []\n",
        "\n",
        "    #Training set\n",
        "    for _ in range(train_size//2): #Generate the negative instances\n",
        "        bag = []\n",
        "        #True MIL should not be able to learn to use this poison item!\n",
        "        poison = np.random.normal(-10, 0.1, size=d)\n",
        "        bag.append(poison)\n",
        "        #Background items contain no info\n",
        "        if min_size is None:\n",
        "          for i in range(np.random.randint(1, 10)):\n",
        "            bag.append(np.random.normal(0.0, 1.0, size=d))\n",
        "        else:\n",
        "          while(len(bag) < min_size):\n",
        "            bag.append(np.random.normal(0.0, 1.0, size=d))\n",
        "\n",
        "        labels_train.append(0)\n",
        "        bags_train.append(np.vstack(bag))\n",
        "\n",
        "    for _ in range(train_size//2): #Generate the positive instances\n",
        "        bag = []\n",
        "\n",
        "\n",
        "        #poisitive indiactors\n",
        "        for i in range(np.random.randint(1, 4)):\n",
        "            if np.random.randint(0,1) == 0:#Subtle positive indiactor\n",
        "                bag.append(np.random.normal(0.0, 3.0, size=d))\n",
        "            else: #more obv positive indiactor\n",
        "                bag.append(np.random.normal(1.0, 1.0, size=d))\n",
        "\n",
        "\n",
        "        #Background items contain no info\n",
        "        if min_size is None:\n",
        "          for i in range(np.random.randint(1, 5)):\n",
        "            bag.append(np.random.normal(0.0, 1.0, size=d))\n",
        "        else:\n",
        "          while len(bag) < min_size:\n",
        "            bag.append(np.random.normal(0.0, 1.0, size=d))\n",
        "\n",
        "        labels_train.append(1)\n",
        "        bags_train.append(np.vstack(bag))\n",
        "\n",
        "    #Test set, poison gets put in the _positive_ bags now, which should cause no issues if using a real MIL model\n",
        "    labels_test = []\n",
        "    bags_test = []\n",
        "    for _ in range(test_size//2): #Generate the negative instances\n",
        "        bag = []\n",
        "        #Background items contain no info\n",
        "        if min_size is None:\n",
        "          for i in range(np.random.randint(1, 10)):\n",
        "            bag.append(np.random.normal(0.0, 1.0, size=d))\n",
        "        else:\n",
        "          while(len(bag) < min_size):\n",
        "            bag.append(np.random.normal(0.0, 1.0, size=d))\n",
        "\n",
        "\n",
        "        labels_test.append(0)\n",
        "        bags_test.append(np.vstack(bag))\n",
        "\n",
        "    for _ in range(test_size//2): #Generate the positive instances\n",
        "        bag = []\n",
        "\n",
        "        #True MIL should not be able to learn to use this poison item!\n",
        "        poison = np.random.normal(-10, 0.1, size=d)\n",
        "        bag.append(poison)\n",
        "\n",
        "        #poisitive indiactors\n",
        "        for i in range(np.random.randint(1, 4)):\n",
        "            if np.random.randint(0,1) == 0:#Subtle positive indiactor\n",
        "                bag.append(np.random.normal(0.0, 3.0, size=d))\n",
        "            else: #more obv positive indiactor\n",
        "                bag.append(np.random.normal(1.0, 1.0, size=d))\n",
        "\n",
        "        #Background items contain no info\n",
        "        if min_size is None:\n",
        "          for i in range(np.random.randint(1, 5)):\n",
        "            bag.append(np.random.normal(0.0, 1.0, size=d))\n",
        "        else:\n",
        "          while len(bag) < min_size:\n",
        "            bag.append(np.random.normal(0.0, 1.0, size=d))\n",
        "\n",
        "        labels_test.append(1)\n",
        "        bags_test.append(np.vstack(bag))\n",
        "\n",
        "    return bags_train, np.array(labels_train, dtype=np.float32), bags_test, np.array(labels_test, dtype=np.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXNq7iA_GNgm"
      },
      "outputs": [],
      "source": [
        "# Algorithm 2 Multi-Concept Standard-MIL\n",
        "# The \"v2\" argument tests a change in the ratio of options put in the bag, see OpenReview discussion.\n",
        "def falseConceptPairMIL(train_size, test_size, min_size=None, d=16, v2=False):\n",
        "    labels_train = []\n",
        "    bags_train = []\n",
        "\n",
        "    #Training set\n",
        "    for _ in range(train_size//2): #Generate the negative instances\n",
        "        bag = []\n",
        "\n",
        "        #True MIL should not be able to learn to use this poison item!\n",
        "        poison = np.random.normal(-10, 0.1, size=d)\n",
        "        bag.append(poison)\n",
        "\n",
        "        #include only one of A or B, but not both\n",
        "        choice = np.random.randint(0, 2)\n",
        "        if v2:\n",
        "          choice = np.random.randint(0, 3)\n",
        "        if choice == 0:\n",
        "            #Item A\n",
        "            bag.append(np.random.normal(2, 0.1, size=d))\n",
        "        #Item B\n",
        "        elif choice == 1:\n",
        "            bag.append(np.random.normal(3, 0.1, size=d))\n",
        "\n",
        "        #Background items contain no info\n",
        "        if min_size is None:\n",
        "          for i in range(np.random.randint(1, 5)):\n",
        "            bag.append(np.random.normal(0.0, 1.0, size=d))\n",
        "        else:\n",
        "          while len(bag) < min_size:\n",
        "            bag.append(np.random.normal(0.0, 1.0, size=d))\n",
        "\n",
        "        labels_train.append(0)\n",
        "        bags_train.append(np.vstack(bag))\n",
        "\n",
        "    for _ in range(train_size//2): #Generate the positive instances\n",
        "        bag = []\n",
        "\n",
        "        #True MIL should not be able to learn to use this poison item!\n",
        "        if np.random.randint(0, 10) == 0:\n",
        "          poison = np.random.normal(-10, 0.1, size=d)\n",
        "          bag.append(poison)\n",
        "\n",
        "        #Item A\n",
        "        for i in range(np.random.randint(1, 5)):\n",
        "            bag.append(np.random.normal(2, 0.1, size=d))\n",
        "        #Item B\n",
        "        for i in range(np.random.randint(1, 5)):\n",
        "            bag.append(np.random.normal(3, 0.1, size=d))\n",
        "\n",
        "        #Background items contain no info\n",
        "        if min_size is None:\n",
        "          for i in range(np.random.randint(1, 5)):\n",
        "            bag.append(np.random.normal(0.0, 1.0, size=d))\n",
        "        else:\n",
        "          while len(bag) < min_size:\n",
        "            bag.append(np.random.normal(0.0, 1.0, size=d))\n",
        "\n",
        "        labels_train.append(1)\n",
        "        bags_train.append(np.vstack(bag))\n",
        "\n",
        "    #Test set, negative instnaces have way too many copies of just _one_ item, but not both. Should still be negative, unless learned count instead of groups\n",
        "    labels_test = []\n",
        "    bags_test = []\n",
        "    for _ in range(test_size//2): #Generate the negative instances\n",
        "        bag = []\n",
        "\n",
        "        #include only one of A or B, but not both\n",
        "        choice = np.random.randint(0, 2)\n",
        "        if choice == 0:\n",
        "            #Item A\n",
        "            bag.append(np.random.normal(2, 0.1, size=d))\n",
        "        #Item B\n",
        "        elif choice == 1:\n",
        "            bag.append(np.random.normal(3, 0.1, size=d))\n",
        "\n",
        "        #Background items contain no info\n",
        "        if min_size is None:\n",
        "          for i in range(np.random.randint(1, 5)):\n",
        "            bag.append(np.random.normal(0.0, 1.0, size=d))\n",
        "        else:\n",
        "          while len(bag) < min_size:\n",
        "            bag.append(np.random.normal(0.0, 1.0, size=d))\n",
        "\n",
        "        labels_test.append(0)\n",
        "        bags_test.append(np.vstack(bag))\n",
        "\n",
        "    for _ in range(test_size//2): #Generate the positive instances\n",
        "        bag = []\n",
        "\n",
        "        #True MIL should not be able to learn to use this poison item!\n",
        "        poison = np.random.normal(-10, 0.1, size=d)\n",
        "        bag.append(poison)\n",
        "\n",
        "        #Item A\n",
        "        for i in range(np.random.randint(1, 5)):\n",
        "            bag.append(np.random.normal(2, 0.1, size=d))\n",
        "        #Item B\n",
        "        for i in range(np.random.randint(1, 5)):\n",
        "            bag.append(np.random.normal(3, 0.1, size=d))\n",
        "\n",
        "        #Background items contain no info\n",
        "        if min_size is None:\n",
        "          for i in range(np.random.randint(1, 5)):\n",
        "            bag.append(np.random.normal(0.0, 1.0, size=d))\n",
        "        else:\n",
        "          while len(bag) < min_size:\n",
        "            bag.append(np.random.normal(0.0, 1.0, size=d))\n",
        "\n",
        "        labels_test.append(1)\n",
        "        bags_test.append(np.vstack(bag))\n",
        "\n",
        "    return bags_train, np.array(labels_train, dtype=np.float32), bags_test, np.array(labels_test, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDs7IQDFgrmM"
      },
      "outputs": [],
      "source": [
        "# This is algorithm 3 from the paper.\n",
        "def falseFrequencyMIL(train_size, test_size, min_size=None, d=16):\n",
        "    labels_train = []\n",
        "    bags_train = []\n",
        "\n",
        "    #Training set\n",
        "    for _ in range(train_size//2): #Generate the negative instances\n",
        "        bag = []\n",
        "\n",
        "        #include only one of A or B, but not both\n",
        "        choice = np.random.randint(0, 2)\n",
        "        if choice == 0:\n",
        "          #Item A\n",
        "          for i in range(np.random.randint(1,2)):\n",
        "            bag.append(np.random.normal(-2, 0.1, size=d))\n",
        "        #Item B\n",
        "        elif choice == 1:\n",
        "          for i in range(np.random.randint(1,2)):\n",
        "            bag.append(np.random.normal(2, 0.1, size=d))\n",
        "\n",
        "        #Background items contain no info\n",
        "        if min_size is None:\n",
        "          for i in range(np.random.randint(1, 10)):\n",
        "            bag.append(np.random.normal(0.0, 1.0, size=d))\n",
        "        else:\n",
        "          while(len(bag) < min_size):\n",
        "            bag.append(np.random.normal(0.0, 1.0, size=d))\n",
        "        labels_train.append(0)\n",
        "        bags_train.append(np.vstack(bag))\n",
        "\n",
        "    for _ in range(train_size//2): #Generate the positive instances\n",
        "        bag = []\n",
        "\n",
        "        #Item A\n",
        "        for i in range(np.random.randint(1, 2)):\n",
        "            bag.append(np.random.normal(-2, 0.1, size=d))\n",
        "        #Item B\n",
        "        for i in range(np.random.randint(1, 2)):\n",
        "            bag.append(np.random.normal(2, 0.1, size=d))\n",
        "\n",
        "        #Background items contain no info\n",
        "        if min_size is None:\n",
        "          for i in range(np.random.randint(1, 10)):\n",
        "            bag.append(np.random.normal(0.0, 1.0, size=d))\n",
        "        else:\n",
        "          while(len(bag) < min_size):\n",
        "            bag.append(np.random.normal(0.0, 1.0, size=d))\n",
        "\n",
        "        labels_train.append(1)\n",
        "        bags_train.append(np.vstack(bag))\n",
        "\n",
        "    #Test set, negative instnaces have way too many copies of just _one_ item, but not both. Should still be negative, unless learned count instead of groups\n",
        "    labels_test = []\n",
        "    bags_test = []\n",
        "    for _ in range(test_size//2): #Generate the negative instances\n",
        "        bag = []\n",
        "\n",
        "        if np.random.randint(1, 10) % 2 == 0:\n",
        "          #Item A\n",
        "          for i in range(np.random.randint(35, 40)):\n",
        "            bag.append(np.random.normal(-2, 0.1, size=d))\n",
        "        else:\n",
        "          #Item B\n",
        "          for i in range(np.random.randint(35, 40)):\n",
        "            bag.append(np.random.normal(2, 0.1, size=d))\n",
        "\n",
        "        #Background items contain no info\n",
        "        if min_size is None:\n",
        "          for i in range(np.random.randint(1, 10)):\n",
        "            bag.append(np.random.normal(0.0, 1.0, size=d))\n",
        "        else:\n",
        "          while(len(bag) < min_size):\n",
        "            bag.append(np.random.normal(0.0, 1.0, size=d))\n",
        "\n",
        "        labels_test.append(0)\n",
        "        bags_test.append(np.vstack(bag))\n",
        "\n",
        "    for _ in range(test_size//2): #Generate the positive instances\n",
        "        bag = []\n",
        "\n",
        "        #poisitive indiactors\n",
        "        #Item A\n",
        "        for i in range(np.random.randint(1, 2)):\n",
        "            bag.append(np.random.normal(-2, 0.1, size=d))\n",
        "        #Item B\n",
        "        for i in range(np.random.randint(1, 2)):\n",
        "            bag.append(np.random.normal(2, 0.1, size=d))\n",
        "\n",
        "        #Background items contain no info\n",
        "        if min_size is None:\n",
        "          for i in range(np.random.randint(1, 10)):\n",
        "            bag.append(np.random.normal(0.0, 1.0, size=d))\n",
        "        else:\n",
        "          while(len(bag) < min_size):\n",
        "            bag.append(np.random.normal(0.0, 1.0, size=d))\n",
        "\n",
        "        labels_test.append(1)\n",
        "        bags_test.append(np.vstack(bag))\n",
        "\n",
        "    return bags_train, np.array(labels_train, dtype=np.float32), bags_test, np.array(labels_test, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfgHrqnugrmM"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ix1XWkh2grmN"
      },
      "outputs": [],
      "source": [
        "class MILScaler():\n",
        "    def __init__(self, scaler):\n",
        "        self.scaler = scaler\n",
        "\n",
        "    def fit(self, X):\n",
        "        return self.scaler.fit(np.vstack(X))\n",
        "\n",
        "    def transform(self, X):\n",
        "        return [self.scaler.transform(x) for x in X]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJhsdp5LPX7u"
      },
      "outputs": [],
      "source": [
        "# X_train, y_train, X_test, y_test = falseNegativeMIL(1000,1000)#, min_size=min_size)\n",
        "# X_train, y_train, X_test, y_test =  falseFrequencyMIL(1000,1000)\n",
        "X_train, y_train, X_test, y_test = falseConceptPairMIL(200,1000, v2=True)\n",
        "\n",
        "names = [\n",
        "    \"mi-SVM\", \"MI-SVM\", \"SIL\", \"NSK\", \"STK\",\n",
        "         \"MICA\", \"MissSVM\"\n",
        "         ]\n",
        "gamma = 0.1000\n",
        "C = 1.0\n",
        "models = [\n",
        "  misvm.miSVM(kernel='linear', C=C, gamma=gamma, scale_C=False, max_iters=250, verbose =False),\n",
        "  misvm.MISVM(kernel='linear', C=C, gamma=gamma, scale_C=False, max_iters=250, verbose =False),\n",
        "  misvm.SIL(kernel='linear', C=C,  gamma=gamma, scale_C=False, verbose =False, sv_cutoff=1e-15),\n",
        "  misvm.NSK(kernel='linear', C=C,  gamma=gamma, scale_C=False, verbose =False, sv_cutoff=1e-15),\n",
        "  misvm.STK(kernel='linear', C=C, gamma=gamma, scale_C=False, verbose =False, sv_cutoff=1e-15),\n",
        "  # misvm.MICA(kernel='linear', C=1.0, gamma=gamma, scale_C=False, max_iters=250, verbose =False),\n",
        "  # misvm.MissSVM(kernel='linear', C=1.0, gamma=gamma, scale_C=False, max_iters=250, verbose =False)\n",
        "\n",
        "  # misvm.miSVM(kernel='rbf', C=C, gamma=gamma, scale_C=False, max_iters=250, verbose =False),\n",
        "  # misvm.MISVM(kernel='rbf', C=C, gamma=gamma, scale_C=False, max_iters=250, verbose =False),\n",
        "  # misvm.SIL(kernel='rbf', C=C,  gamma=gamma, scale_C=False, verbose =False),\n",
        "  # misvm.NSK(kernel='rbf', C=C,  gamma=gamma, scale_C=False, verbose =False),\n",
        "  # misvm.STK(kernel='rbf', C=C, gamma=gamma, scale_C=False, verbose =False),\n",
        "  # misvm.MICA(kernel='rbf', C=1.0, gamma=gamma, scale_C=False, max_iters=250, verbose =False),\n",
        "  # misvm.MissSVM(kernel='rbf', C=1.0, gamma=gamma, scale_C=False, max_iters=250, verbose =False)\n",
        "]\n",
        "\n",
        "for name, classifier in zip(names, models):\n",
        "  classifier.fit(X_train, y_train*2-1)\n",
        "  preds_train = classifier.predict(X_train)\n",
        "  preds = classifier.predict(X_test)\n",
        "\n",
        "  result = {\n",
        "    \"Train: Balanced Accuracy\":balanced_accuracy_score(y_train, np.round((np.sign(preds_train)+1)/2)),\n",
        "    \"Train: Accuracy\":accuracy_score(y_train, np.round((np.sign(preds_train)+1)/2)),\n",
        "    \"Train: F1\":f1_score(y_train, np.round((np.sign(preds_train)+1)/2)),\n",
        "    \"Train: AUC\":roc_auc_score(y_train, preds_train),\n",
        "    #####\n",
        "    \"Balanced Accuracy\":balanced_accuracy_score(y_test, np.round((np.sign(preds)+1)/2)),\n",
        "    \"Accuracy\":accuracy_score(y_test, np.round((np.sign(preds)+1)/2)),\n",
        "    \"F1\":f1_score(y_test, np.round((np.sign(preds)+1)/2)),\n",
        "    \"AUC\":roc_auc_score(y_test, preds),\n",
        "    }\n",
        "  print(name, result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2G5H1A_grmN"
      },
      "outputs": [],
      "source": [
        "class MILDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, X, y, scaler=None):\n",
        "\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        if scaler is None:\n",
        "            self.scaler = StandardScaler()\n",
        "#             self.scaler = MinMaxScaler()\n",
        "            self.scaler.fit(np.vstack(X))\n",
        "        else:\n",
        "            self.scaler = scaler\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        x = self.scaler.transform(np.vstack(self.X[idx]))\n",
        "        y = int(self.y[idx])\n",
        "\n",
        "        return [torch.tensor(q, dtype=torch.float32) for q in x], torch.tensor(y, dtype=torch.int64)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqxkkEUygrmN"
      },
      "outputs": [],
      "source": [
        "max_trials = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XmYam7BgrmN"
      },
      "outputs": [],
      "source": [
        "NUM_GPUS = 1\n",
        "MAX_BAG_SIZE = 5000//4  #trim all bags to this many strings\n",
        "MAX_BATCH_STRINGS = 5000 #max desired number of effective strings in a batch (includes padding strings! )\n",
        "MAX_BATCH_SIZE = 32\n",
        "# MAX_BATCH_SIZE = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NihMoZRGgrmN"
      },
      "outputs": [],
      "source": [
        "class VariableBatchSampler(torch.utils.data.Sampler):\n",
        "    \"\"\"Wraps another sampler to yield a mini-batch of indices.\n",
        "\n",
        "    Args:\n",
        "        sampler (Sampler): Base sampler.\n",
        "        batch_size (int): Size of mini-batch.\n",
        "        drop_last (bool): If ``True``, the sampler will drop the last batch if\n",
        "            its size would be less than ``batch_size``\n",
        "\n",
        "    Example:\n",
        "        >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False))\n",
        "        [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n",
        "        >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True))\n",
        "        [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, bag_sizes, NUM_GPUS=1, max_batch_size=32):\n",
        "        \"\"\"\n",
        "        bag_sizes: for eah index i in a source dataset, bag_sizes[i] should\n",
        "            indicate the number of strings in that bag.\n",
        "        \"\"\"\n",
        "\n",
        "        finished = set()\n",
        "\n",
        "        iter_order = np.argsort(bag_sizes)[::-1] #reverse order, big items first please\n",
        "\n",
        "\n",
        "        #grab single items that are already at the max size, will be a batch size of 1\n",
        "        #finished.update(iter_order[bag_sizes[iter_order] >= MAX_BATCH_STRINGS].tolist())\n",
        "\n",
        "        #Now lets grab the leftover items\n",
        "        iter_order = iter_order[bag_sizes[iter_order] < MAX_BATCH_STRINGS]\n",
        "\n",
        "        final_bag_batches = [ ]\n",
        "\n",
        "        #a heap as tuples (priority, value)\n",
        "        #priority is the bagBatchSize, smallest bag batches will come out first\n",
        "        to_improve = [ ( -min(bag_sizes[i],MAX_BAG_SIZE) , i) for i in iter_order]\n",
        "        heapq.heapify(to_improve)\n",
        "\n",
        "        #\"Rounded\" size, where we round based on numb GPUs and clip to MAX_BAG_SIZE and MAX_BATCH_STRINGS/NUM_GPUS\n",
        "        #This ensures that the batches we create can be distributed cleanly accros NUM_GPUS without any bag's\n",
        "        #contents stretching across a GPU boundary\n",
        "        def rSize(x):\n",
        "            return min(math.ceil(x/float(NUM_GPUS))*NUM_GPUS, MAX_BAG_SIZE, MAX_BATCH_STRINGS//NUM_GPUS)\n",
        "\n",
        "        while True and len(to_improve) > 0:\n",
        "            _, toAdd = to_improve[0]\n",
        "            cur_list = [toAdd]\n",
        "            heapq.heappop(to_improve)\n",
        "\n",
        "            #While we can add one more smaller bag, to it\n",
        "            while len(cur_list) < max_batch_size and len(to_improve) > 0 and rSize(max(bag_sizes[cur_list]))*rSize(len(cur_list)+1) <= MAX_BATCH_STRINGS:\n",
        "                _, toAdd = to_improve[0]\n",
        "                cur_list = cur_list + [toAdd]\n",
        "                heapq.heappop(to_improve)\n",
        "            finished.update(cur_list)\n",
        "            final_bag_batches.append( cur_list )\n",
        "        self.final_bag_batches = final_bag_batches\n",
        "\n",
        "    def __iter__(self):\n",
        "        order = list(range(len(self)))\n",
        "        random.shuffle(order)\n",
        "        for i in order:\n",
        "#             print(self.final_bag_batches[i])\n",
        "            yield self.final_bag_batches[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.final_bag_batches)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paX7w7JFgrmO"
      },
      "outputs": [],
      "source": [
        "PAD_VAL = 0\n",
        "\n",
        "def randomSlice(arr, max_size):\n",
        "    if len(arr) <= max_size:\n",
        "        return np.pad(arr,(0,max_size-len(arr)),'constant', constant_values=(PAD_VAL, PAD_VAL))[0:max_size]\n",
        "        return arr\n",
        "    start = np.random.randint(0,len(arr)-max_size)\n",
        "    return (arr[start:start+max_size])[0:max_size]\n",
        "\n",
        "def randomSubset(bag, max_size):\n",
        "    if len(bag) <= max_size:\n",
        "        return bag\n",
        "\n",
        "    return np.random.choice(bag, min(len(bag),max_size), replace=False)\n",
        "\n",
        "ignore_index = -100 #Default for softmax cross entropy\n",
        "\n",
        "def collate_fn(batch):\n",
        "    #X = [randomSubset(bag,MAX_BAG_SIZE) for bag, _ in batch]\n",
        "    X = [bag for bag, _ in batch]\n",
        "    Y = [y for _, y in batch]\n",
        "\n",
        "    #longest string within the batch, clipped to max we wil allow\n",
        "    batch_max = max([max([len(s) for s in bag]) for bag, _ in batch])\n",
        "\n",
        "    #We want the first index of the array to be a generic empy vector for use in the model's inference process\n",
        "    #we are doing that here to maximize GPU util so this can be done in advance and not be a GPU op that is\n",
        "    #almost pure overhead costs\n",
        "    empty = np.array([PAD_VAL]*batch_max, dtype=np.float32)\n",
        "\n",
        "    #Figure out how many items are in the biggest bag, and padd it out based on num GPUs\n",
        "    bag_max_size = max([len(x) for x in X])\n",
        "    bag_max_size = math.ceil(bag_max_size/float(NUM_GPUS))*NUM_GPUS\n",
        "    #pad the bags so that we can evenly split the bag into parts\n",
        "    for x in X:\n",
        "        while len(x) < bag_max_size:\n",
        "            x.append( empty )\n",
        "    #Pad out the batch itself!\n",
        "    while len(X) != math.ceil(len(X)/float(NUM_GPUS))*NUM_GPUS:\n",
        "        X.append([empty]*bag_max_size)\n",
        "        Y.append(ignore_index)\n",
        "    B = len(X)\n",
        "\n",
        "    X = np.vstack(\n",
        "    [\n",
        "        np.vstack([randomSlice(s, batch_max) for s in bag])\n",
        "        for bag in X]\n",
        "    )\n",
        "\n",
        "    batch_starts = np.array([i*bag_max_size for i in range(B)], dtype=np.int64)\n",
        "    return X, batch_starts, Y #, weird"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgOoMWWygrmO"
      },
      "outputs": [],
      "source": [
        "# collate_fn([mildata[2], mildata[72]])[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okpConjXgrmO"
      },
      "outputs": [],
      "source": [
        "class AttentionAvg(nn.Module):\n",
        "\n",
        "    def __init__(self, attnScore):\n",
        "        super(AttentionAvg, self).__init__()\n",
        "        self.score = attnScore\n",
        "\n",
        "    def forward(self, states, context=None, mask=None):\n",
        "        \"\"\"\n",
        "        states: (B, T, D) shape\n",
        "        context: (B, D) shape\n",
        "        output: (B, D), a weighted av\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        B = states.size(0)\n",
        "        T = states.size(1)\n",
        "        D = states.size(2)\n",
        "\n",
        "        #If we were not given a context, but we do have a mask, create context ourselves\n",
        "        if context is None and mask is not None:\n",
        "            context = states.sum(dim=1)/(mask.sum(dim=1).unsqueeze(1)+1e-5) #(B, D)\n",
        "\n",
        "        scores = self.score(states, context) #(B, T, 1)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores[~mask] = float(-10000)\n",
        "        weights = F.softmax(scores, dim=1) #(B, T, 1) still, but sum(T) = 1\n",
        "\n",
        "        context = (states*weights).sum(dim=1) #(B, T, D) * (B, T, 1) -> (B, D, 1)\n",
        "\n",
        "#         [z for z in X.shape[0:-2]] + [X.size(-1)]\n",
        "        return context.view(B, D) #Flatten this out to (B, D)\n",
        "\n",
        "\n",
        "class AdditiveAttentionScore(nn.Module):\n",
        "\n",
        "    def __init__(self, D):\n",
        "        super(AdditiveAttentionScore, self).__init__()\n",
        "        self.v = nn.Linear(D, 1)\n",
        "        self.w = nn.Linear(2*D, D)\n",
        "\n",
        "    def forward(self, states, context):\n",
        "        \"\"\"\n",
        "        states:  (B, *, T, D) shape\n",
        "        context: (B, *, D) shape\n",
        "        output:  (B, *, T, 1), giving a score to each of the T items based on the context D\n",
        "\n",
        "        \"\"\"\n",
        "        T = states.size(-2)\n",
        "        #Repeating the values T times\n",
        "        context = torch.stack([context for _ in range(T)], dim=len(context.shape)-1) #(B, *, D) -> (B, *, T, D)\n",
        "        state_context_combined = torch.cat((states, context), dim=-1) #(B, *, T, D) + (B, *, T, D)  -> (B, *, T, 2*D)\n",
        "        scores = self.v(torch.tanh(self.w(state_context_combined)))\n",
        "        return scores\n",
        "\n",
        "class GeneralScore(nn.Module):\n",
        "\n",
        "    def __init__(self, D):\n",
        "        super(GeneralScore, self).__init__()\n",
        "        self.w = nn.Bilinear(D, D, 1)\n",
        "\n",
        "    def forward(self, states, context):\n",
        "        \"\"\"\n",
        "        states: (B, T, D) shape\n",
        "        context: (B, D) shape\n",
        "        output: (B, T, 1), giving a score to each of the T items based on the context D\n",
        "\n",
        "        \"\"\"\n",
        "        T = states.size(1)\n",
        "        D = states.size(2)\n",
        "        #Repeating the values T times\n",
        "        context = torch.stack([context for _ in range(T)], dim=1) #(B, D) -> (B, T, D)\n",
        "        scores = self.w(states, context) #(B, T, D) -> (B, T, 1)\n",
        "        return scores\n",
        "\n",
        "class DotScore(nn.Module):\n",
        "\n",
        "    def __init__(self, D):\n",
        "        super(DotScore, self).__init__()\n",
        "\n",
        "    def forward(self, states, context):\n",
        "        \"\"\"\n",
        "        states: (B, T, D) shape\n",
        "        context: (B, D) shape\n",
        "        output: (B, T, 1), giving a score to each of the T items based on the context D\n",
        "\n",
        "        \"\"\"\n",
        "        T = states.size(1)\n",
        "        D = states.size(2)\n",
        "\n",
        "        scores = torch.bm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNjUYj7YgrmO"
      },
      "outputs": [],
      "source": [
        "class View(nn.Module):\n",
        "    def __init__(self, *shape):\n",
        "        super(View, self).__init__()\n",
        "        self.shape = shape\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input.view(*self.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_r7yDdugrmO"
      },
      "outputs": [],
      "source": [
        "def getMaskByFill(x, time_dimension=1, fill=0):\n",
        "    \"\"\"\n",
        "    x: the original input with three or more dimensions, (B, ..., T, ...)\n",
        "        which may have unsued items in the tensor. B is the batch size,\n",
        "        and T is the time dimension.\n",
        "    time_dimension: the axis in the tensor `x` that denotes the time dimension\n",
        "    fill: the constant used to denote that an item in the tensor is not in use,\n",
        "        and should be masked out (`False` in the mask).\n",
        "\n",
        "    return: A boolean tensor of shape (B, T), where `True` indicates the value\n",
        "        at that time is good to use, and `False` that it is not.\n",
        "    \"\"\"\n",
        "    to_sum_over = list(range(1,len(x.shape))) #skip the first dimension 0 because that is the batch dimension\n",
        "\n",
        "    if time_dimension in to_sum_over:\n",
        "        to_sum_over.remove(time_dimension)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        #Special case is when shape is (B, D), then it is an embedding layer. We just return the values that are good\n",
        "        if len(to_sum_over) == 0:\n",
        "            return (x != fill)\n",
        "        #(x!=fill) determines locations that might be unused, beause they are\n",
        "        #missing the fill value we are looking for to indicate lack of use.\n",
        "        #We then count the number of non-fill values over everything in that\n",
        "        #time slot (reducing changes the shape to (B, T)). If any one entry\n",
        "        #is non equal to this value, the item represent must be in use -\n",
        "        #so return a value of true.\n",
        "        mask = torch.sum((x != fill), dim=to_sum_over) > 0\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqcV5PpxgrmP"
      },
      "outputs": [],
      "source": [
        "class BatchModel(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, D, out_size=2):\n",
        "        super(BatchModel, self).__init__()\n",
        "        leak=0.2\n",
        "        self.emb = nn.Sequential(\n",
        "            nn.Linear(d_in, D),\n",
        "            nn.LeakyReLU(leak),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(D, D),\n",
        "            nn.LeakyReLU(leak),\n",
        "            nn.Dropout(),\n",
        "        )\n",
        "        self.attn = AttentionAvg(AdditiveAttentionScore(D))\n",
        "        self.pred = nn.Sequential(\n",
        "            nn.Linear(D, D),\n",
        "            nn.LeakyReLU(leak),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(D, D),\n",
        "            nn.LeakyReLU(leak),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(D, out_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, input_strings, batch_starts):\n",
        "        \"\"\"\n",
        "        input_strings: a tensor of shape (S+1, T), where S is the total number of strings in each bag,\n",
        "            and T is the length of the longest string. The first row should contain only padding values.\n",
        "        batch_starts: a tensor of shape (B+1), where B is the total number of bags/batch size.\n",
        "            For 0 <= i <= B, batch_starts[i] should contain the index in `input_strings` that the i'th\n",
        "            bag starts at. Everything between [batch_starts[i], batch_starts[i+1]) should belong to the\n",
        "            i'th bag. The last index at batch_starts[B] should be the value S+1.\n",
        "        \"\"\"\n",
        "\n",
        "        device = input_strings.device\n",
        "        z = self.emb(input_strings) #(S, D') -> (S, D)\n",
        "\n",
        "        bag_sizes = batch_starts[1:]-batch_starts[0:-1] # (B)\n",
        "        max_bag_size = torch.max(bag_sizes) #We are going to pad out smaller bags to the largest bag size\n",
        "\n",
        "        ###\n",
        "        #z is currently shaped (S, D) which is (B*L, D), B = batch_starts.size\n",
        "        #So can view as (B, L, D)\n",
        "        new_batch = z.view(batch_starts.size(0), -1, z.size(1))\n",
        "\n",
        "        mask = getMaskByFill(new_batch)\n",
        "\n",
        "        z = self.attn(new_batch, mask=mask) #(B, D)\n",
        "\n",
        "        return self.pred(z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJ7qqEi0grmP"
      },
      "outputs": [],
      "source": [
        "class MIL_Pool(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, D, out_size=1):\n",
        "        super(MIL_Pool, self).__init__()\n",
        "        leak=0.2\n",
        "        self.emb = nn.Sequential(\n",
        "            nn.Linear(d_in, D),\n",
        "            nn.LeakyReLU(leak),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(D, D),\n",
        "            nn.LeakyReLU(leak),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(D, D),\n",
        "            nn.LeakyReLU(leak),\n",
        "            nn.Dropout(),\n",
        "            # nn.Linear(D, out_size),\n",
        "            # nn.LeakyReLU(leak),\n",
        "            # nn.Dropout(),\n",
        "        )\n",
        "        self.attn = AttentionAvg(AdditiveAttentionScore(D))\n",
        "        self.pred = nn.Sequential(\n",
        "#             nn.Linear(D, D),\n",
        "#             nn.LeakyReLU(leak),\n",
        "#             nn.Dropout(),\n",
        "#             nn.Linear(D, D),\n",
        "#             nn.LeakyReLU(leak),\n",
        "#             nn.Dropout(),\n",
        "            nn.Linear(D, out_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, input_strings, batch_starts):\n",
        "        \"\"\"\n",
        "        input_strings: a tensor of shape (S+1, T), where S is the total number of strings in each bag,\n",
        "            and T is the length of the longest string. The first row should contain only padding values.\n",
        "        batch_starts: a tensor of shape (B+1), where B is the total number of bags/batch size.\n",
        "            For 0 <= i <= B, batch_starts[i] should contain the index in `input_strings` that the i'th\n",
        "            bag starts at. Everything between [batch_starts[i], batch_starts[i+1]) should belong to the\n",
        "            i'th bag. The last index at batch_starts[B] should be the value S+1.\n",
        "        \"\"\"\n",
        "\n",
        "        device = input_strings.device\n",
        "        z = self.emb(input_strings) #(S, D') -> (S, D)\n",
        "        # print(z.shape)\n",
        "        bag_sizes = batch_starts[1:]-batch_starts[0:-1] # (B)\n",
        "        max_bag_size = torch.max(bag_sizes) #We are going to pad out smaller bags to the largest bag size\n",
        "        # print(bag_sizes, max_bag_size)\n",
        "        ###\n",
        "        #z is currently shaped (S, D) which is (B*L, D), B = batch_starts.size\n",
        "        #So can view as (B, L, D)\n",
        "        new_batch = z.view(batch_starts.size(0), -1, z.size(-1))\n",
        "        # print(new_batch.shape)\n",
        "        mask = getMaskByFill(new_batch)\n",
        "\n",
        "        new_batch = torch.where(mask.view(batch_starts.size(0), -1, 1), new_batch, torch.full_like(new_batch, -1000.0))\n",
        "\n",
        "        # print(new_batch.permute(0, 2, 1).shape)\n",
        "        new_batch = self.attn(new_batch, mask=mask) #(B, D)\n",
        "        # new_batch = F.adaptive_max_pool1d(z.permute(0, 2, 1), 1).view(-1, 1)\n",
        "        return new_batch.sum(-1).view(-1, 1)\n",
        "\n",
        "#         print(new_batch.shape)\n",
        "\n",
        "        # return z #self.pred(z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnqO_dRqDml7"
      },
      "outputs": [],
      "source": [
        "class miNet(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, D):\n",
        "        super(miNet, self).__init__()\n",
        "        leak=0.2\n",
        "        self.emb = nn.Sequential(\n",
        "            nn.Linear(d_in, D),\n",
        "            nn.LeakyReLU(leak),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(D, D),\n",
        "            nn.LeakyReLU(leak),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(D, D),\n",
        "            nn.LeakyReLU(leak),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(D, 1)\n",
        "        )\n",
        "\n",
        "        # self.pred = nn.Sequential(\n",
        "        #     nn.Linear(D, 1)\n",
        "        # )\n",
        "\n",
        "    def forward(self, input_strings, batch_starts):\n",
        "        \"\"\"\n",
        "        input_strings: a tensor of shape (S+1, T), where S is the total number of strings in each bag,\n",
        "            and T is the length of the longest string. The first row should contain only padding values.\n",
        "        batch_starts: a tensor of shape (B+1), where B is the total number of bags/batch size.\n",
        "            For 0 <= i <= B, batch_starts[i] should contain the index in `input_strings` that the i'th\n",
        "            bag starts at. Everything between [batch_starts[i], batch_starts[i+1]) should belong to the\n",
        "            i'th bag. The last index at batch_starts[B] should be the value S+1.\n",
        "        \"\"\"\n",
        "\n",
        "        device = input_strings.device\n",
        "        z = self.emb(input_strings) #(S, D') -> (S, 1)\n",
        "\n",
        "        bag_sizes = batch_starts[1:]-batch_starts[0:-1] # (B)\n",
        "        max_bag_size = torch.max(bag_sizes) #We are going to pad out smaller bags to the largest bag size\n",
        "\n",
        "        ###\n",
        "        #z is currently shaped (S, D) which is (B*L, D), B = batch_starts.size\n",
        "        #So can view as (B, L, D)\n",
        "        new_batch = z.view(batch_starts.size(0), -1, z.size(1))\n",
        "\n",
        "        mask = getMaskByFill(new_batch)\n",
        "\n",
        "        final_context = F.adaptive_max_pool1d(new_batch.permute(0, 2, 1), 1).squeeze() #(B, T, D) -> (B, D)\n",
        "\n",
        "        return final_context.view(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRiidE7zgrmP"
      },
      "outputs": [],
      "source": [
        "class MiNet(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, D):\n",
        "        super(MiNet, self).__init__()\n",
        "        leak=0.2\n",
        "        self.emb = nn.Sequential(\n",
        "            nn.Linear(d_in, D),\n",
        "            nn.LeakyReLU(leak),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(D, D),\n",
        "            nn.LeakyReLU(leak),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(D, D),\n",
        "            nn.LeakyReLU(leak),\n",
        "            nn.Dropout(),\n",
        "        )\n",
        "\n",
        "        self.pred = nn.Sequential(\n",
        "            nn.Linear(D, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_strings, batch_starts):\n",
        "        \"\"\"\n",
        "        input_strings: a tensor of shape (S+1, T), where S is the total number of strings in each bag,\n",
        "            and T is the length of the longest string. The first row should contain only padding values.\n",
        "        batch_starts: a tensor of shape (B+1), where B is the total number of bags/batch size.\n",
        "            For 0 <= i <= B, batch_starts[i] should contain the index in `input_strings` that the i'th\n",
        "            bag starts at. Everything between [batch_starts[i], batch_starts[i+1]) should belong to the\n",
        "            i'th bag. The last index at batch_starts[B] should be the value S+1.\n",
        "        \"\"\"\n",
        "\n",
        "        device = input_strings.device\n",
        "        z = self.emb(input_strings) #(S, D') -> (S, D)\n",
        "\n",
        "        bag_sizes = batch_starts[1:]-batch_starts[0:-1] # (B)\n",
        "        max_bag_size = torch.max(bag_sizes) #We are going to pad out smaller bags to the largest bag size\n",
        "\n",
        "        ###\n",
        "        #z is currently shaped (S, D) which is (B*L, D), B = batch_starts.size\n",
        "        #So can view as (B, L, D)\n",
        "        new_batch = z.view(batch_starts.size(0), -1, z.size(1))\n",
        "\n",
        "        mask = getMaskByFill(new_batch)\n",
        "\n",
        "        final_context = F.adaptive_max_pool1d(new_batch.permute(0, 2, 1), 1).squeeze() #(B, T, D) -> (B, D)\n",
        "\n",
        "        return self.pred(final_context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQy5EbTYuyyQ"
      },
      "outputs": [],
      "source": [
        "class TranMil(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, D):\n",
        "        super(TranMil, self).__init__()\n",
        "        leak=0.2\n",
        "        self.emb = nn.Sequential(\n",
        "            nn.Linear(d_in, D),\n",
        "        )\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.TransformerEncoderLayer(d_model=D, dim_feedforward=D, nhead=8, batch_first=True),\n",
        "            nn.TransformerEncoderLayer(d_model=D, dim_feedforward=D, nhead=8, batch_first=True),\n",
        "            nn.TransformerEncoderLayer(d_model=D, dim_feedforward=D, nhead=8, batch_first=True),\n",
        "        )\n",
        "\n",
        "        self.pred = nn.Sequential(\n",
        "            nn.Linear(D, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_strings, batch_starts):\n",
        "        \"\"\"\n",
        "        input_strings: a tensor of shape (S+1, T), where S is the total number of strings in each bag,\n",
        "            and T is the length of the longest string. The first row should contain only padding values.\n",
        "        batch_starts: a tensor of shape (B+1), where B is the total number of bags/batch size.\n",
        "            For 0 <= i <= B, batch_starts[i] should contain the index in `input_strings` that the i'th\n",
        "            bag starts at. Everything between [batch_starts[i], batch_starts[i+1]) should belong to the\n",
        "            i'th bag. The last index at batch_starts[B] should be the value S+1.\n",
        "        \"\"\"\n",
        "\n",
        "        device = input_strings.device\n",
        "        z = self.emb(input_strings) #(S, D') -> (S, D)\n",
        "\n",
        "        bag_sizes = batch_starts[1:]-batch_starts[0:-1] # (B)\n",
        "        max_bag_size = torch.max(bag_sizes) #We are going to pad out smaller bags to the largest bag size\n",
        "\n",
        "        ###\n",
        "        #z is currently shaped (S, D) which is (B*L, D), B = batch_starts.size\n",
        "        #So can view as (B, L, D)\n",
        "        new_batch = z.view(batch_starts.size(0), -1, z.size(1))\n",
        "        new_batch = self.layers(new_batch)\n",
        "\n",
        "        mask = getMaskByFill(new_batch)\n",
        "\n",
        "        final_context = F.adaptive_max_pool1d(new_batch.permute(0, 2, 1), 1).squeeze() #(B, T, D) -> (B, D)\n",
        "\n",
        "        return self.pred(final_context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8Gjo1XmgrmP"
      },
      "outputs": [],
      "source": [
        "class LinearNN(nn.Module):\n",
        "    __constants__ = ['in_features', 'out_features']\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(LinearNN, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "        self.weight.data = torch.log(torch.abs(self.weight.data))\n",
        "        if self.bias is not None:\n",
        "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
        "            bound = 1 / np.math.sqrt(fan_in)\n",
        "            nn.init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.linear(input, torch.exp(self.weight), self.bias)\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return 'in_features={}, out_features={}, bias={}'.format(\n",
        "            self.in_features, self.out_features, self.bias is not None\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPDhV2zAgrmQ"
      },
      "outputs": [],
      "source": [
        "class MILAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, D):\n",
        "        super(MILAttention, self).__init__()\n",
        "        self.score = MonoAdditiveAttentionScore(D, D)\n",
        "        self.pool = CountMILPool(D)\n",
        "\n",
        "    def forward(self, states, context=None, mask=None):\n",
        "        \"\"\"\n",
        "        states: (B, T, D) shape\n",
        "        context: (B, D) shape\n",
        "        output: (B, D), a weighted av\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        B = states.size(0)\n",
        "        T = states.size(1)\n",
        "        D = states.size(2)\n",
        "\n",
        "        #If we were not given a context, but we do have a mask, create context ourselves\n",
        "        if context is None and mask is not None:\n",
        "            #context = states.sum(dim=1)/(mask.sum(dim=1).unsqueeze(1)+1e-5) #(B, D)\n",
        "            #context = F.adaptive_max_pool1d(states.permute(0, 2, 1), 1).squeeze() #(B, D)\n",
        "            context = self.pool(states)\n",
        "\n",
        "#         print(\"T:\", states.shape, context.shape)\n",
        "        scores = self.score(states, context) #(B, T, 1)\n",
        "#         print(scores.shape)\n",
        "        if mask is not None:\n",
        "            scores[~mask] = float(-10000)\n",
        "        #weights = F.softmax(scores, dim=1) #(B, T, 1) still, but sum(T) = 1\n",
        "        weights = torch.sigmoid(scores)\n",
        "\n",
        "#         print(states.shape, weights.shape)\n",
        "        final_context = (states*weights) #(B, T, D) * (B, T, 1) -> (B, T, D)\n",
        "        #final_context = F.adaptive_max_pool1d(final_context.permute(0, 2, 1), 1).squeeze() #(B, T, D) -> (B, D)\n",
        "        final_context = self.pool(final_context)\n",
        "\n",
        "#         [z for z in X.shape[0:-2]] + [X.size(-1)]\n",
        "        return final_context.view(B, -1) #Flatten this out to (B, D)\n",
        "#         return context.view(B, D) #Flatten this out to (B, D)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yK3fmHwcgrmR"
      },
      "outputs": [],
      "source": [
        "class NMModel(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, D):\n",
        "        super(NMModel, self).__init__()\n",
        "        leak=0.2\n",
        "        self.emb = nn.Sequential(\n",
        "            nn.Linear(d_in, D),\n",
        "            nn.LeakyReLU(leak),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(D, D),\n",
        "            nn.LeakyReLU(leak),\n",
        "            nn.Dropout(),\n",
        "        )\n",
        "        self.attn = MILAttention(D)\n",
        "        self.pred = MonotonicG(D, 10, D//10)\n",
        "\n",
        "    def forward(self, input_strings, batch_starts):\n",
        "        \"\"\"\n",
        "        input_strings: a tensor of shape (S+1, T), where S is the total number of strings in each bag,\n",
        "            and T is the length of the longest string. The first row should contain only padding values.\n",
        "        batch_starts: a tensor of shape (B+1), where B is the total number of bags/batch size.\n",
        "            For 0 <= i <= B, batch_starts[i] should contain the index in `input_strings` that the i'th\n",
        "            bag starts at. Everything between [batch_starts[i], batch_starts[i+1]) should belong to the\n",
        "            i'th bag. The last index at batch_starts[B] should be the value S+1.\n",
        "        \"\"\"\n",
        "\n",
        "        device = input_strings.device\n",
        "        z = self.emb(input_strings) #(S, D') -> (S, D)\n",
        "\n",
        "        bag_sizes = batch_starts[1:]-batch_starts[0:-1] # (B)\n",
        "        max_bag_size = torch.max(bag_sizes) #We are going to pad out smaller bags to the largest bag size\n",
        "\n",
        "        ###\n",
        "        #z is currently shaped (S, D) which is (B*L, D), B = batch_starts.size\n",
        "        #So can view as (B, L, D)\n",
        "        new_batch = z.view(batch_starts.size(0), -1, z.size(1))\n",
        "\n",
        "        mask = getMaskByFill(new_batch)\n",
        "\n",
        "        z = torch.sigmoid(self.attn(new_batch, mask=mask)) #(B, D)\n",
        "\n",
        "        return self.pred(z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a65HN4kN2qi4"
      },
      "outputs": [],
      "source": [
        "data = torch.randn(7, 10)\n",
        "pair_dists = torch.cdist(data, data)\n",
        "order = torch.topk(pair_dists, 3, largest=False).indices\n",
        "edges = torch.hstack((torch.stack((order[:,0],order[:,1])), torch.stack((order[:,0],order[:,2]))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkdHhtqDgrmQ"
      },
      "outputs": [],
      "source": [
        "class MilGCN(torch.nn.Module):\n",
        "  def __init__(self, d_in, D):\n",
        "    super(MilGCN, self).__init__()\n",
        "    self.conv1 = GCNConv(d_in, D)\n",
        "    self.conv2 = GCNConv(D, D)\n",
        "    self.conv3 = GCNConv(D, D)\n",
        "    self.lin = nn.Linear(D, 1)\n",
        "\n",
        "  def forward(self, input_strings, batch_starts):\n",
        "    batch = 0\n",
        "    device = input_strings.device\n",
        "    x = input_strings #(L, D) - because we are hard coding to a bath size of 1 for this model\n",
        "\n",
        "    # print(z.shape)\n",
        "\n",
        "    # bag_sizes = batch_starts[1:]-batch_starts[0:-1] # (B)\n",
        "    # max_bag_size = torch.max(bag_sizes) #We are going to pad out smaller bags to the largest bag size\n",
        "\n",
        "    ###\n",
        "    #z is currently shaped (S, D) which is (B*L, D), B = batch_starts.size\n",
        "    #So can view as (B, L, D)\n",
        "    # new_batch = z.view(batch_starts.size(0), -1, z.size(1))\n",
        "    x = x\n",
        "    # print(x.shape)\n",
        "    pair_dists = torch.cdist(x, x)\n",
        "    order = torch.topk(pair_dists, 3, largest=False).indices #create graph based on 2-NN\n",
        "    edge_index = torch.hstack((torch.stack((order[:,0],order[:,1])), torch.stack((order[:,0],order[:,2]))))\n",
        "    #X comes in with shape ()\n",
        "    # 1. Obtain node embeddings\n",
        "    x = self.conv1(x, edge_index)\n",
        "    x = x.relu()\n",
        "    x = self.conv2(x, edge_index)\n",
        "    x = x.relu()\n",
        "    x = self.conv3(x, edge_index)\n",
        "\n",
        "    # 2. Readout layer\n",
        "    x = global_mean_pool(x, torch.zeros(x.size(0), dtype=torch.long))  # [batch_size, D]\n",
        "\n",
        "    # 3. Apply a final classifier\n",
        "    x = F.dropout(x, p=0.1, training=self.training)\n",
        "    x = self.lin(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXIXuDAXgrmR"
      },
      "source": [
        "# Run a Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEyrQUd2grmS"
      },
      "outputs": [],
      "source": [
        "epochs = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duZV94qlgrmS"
      },
      "outputs": [],
      "source": [
        "embed_dim = 128\n",
        "hidden_dim = 128\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0L182K5grmS"
      },
      "outputs": [],
      "source": [
        "oneItemOnly = False\n",
        "scores_dataset = {}\n",
        "\n",
        "\n",
        "min_size = 45\n",
        "\n",
        "# X_train, y_train, X_test, y_test = falseNegativeMIL(10000,1000)#, min_size=min_size)\n",
        "# # X_train, y_train, X_test, y_test =  falseFrequencyMIL(10000,1000, min_size=min_size)\n",
        "X_train, y_train, X_test, y_test = falseConceptPairMIL(10000,1000, min_size=min_size, v2=True)\n",
        "\n",
        "X_train_v, X_v, y_train_v, y_v = train_test_split(X_train, y_train,\n",
        "                                                        test_size=0.2, random_state=42, stratify=y_train)\n",
        "\n",
        "# scalar_train = MILScaler(MinMaxScaler())\n",
        "# scalar_train_v = MILScaler(MinMaxScaler())\n",
        "scalar_train = MILScaler(StandardScaler())\n",
        "scalar_train_v = MILScaler(StandardScaler())\n",
        "\n",
        "scalar_train.fit(X_train)\n",
        "scalar_train_v.fit(X_train_v)\n",
        "\n",
        "#     print(len(X_train_v[0]))\n",
        "\n",
        "train_dataset = MILDataset(X_train, y_train)\n",
        "test_dataset = MILDataset(X_test, y_test, scaler=train_dataset.scaler)\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "trainSampler = VariableBatchSampler(np.array([len(x) for x in X_train_v]), max_batch_size=BATCH_SIZE, NUM_GPUS=NUM_GPUS)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, num_workers=2, collate_fn=collate_fn,\n",
        "                     batch_sampler=VariableBatchSampler(np.array([len(x) for x in X_train_v]),\n",
        "                        max_batch_size=BATCH_SIZE, NUM_GPUS=NUM_GPUS))\n",
        "test_loader = DataLoader(test_dataset, num_workers=2, collate_fn=collate_fn,\n",
        "                     batch_sampler=VariableBatchSampler(np.array([len(x) for x in X_test]),\n",
        "                        max_batch_size=BATCH_SIZE, NUM_GPUS=NUM_GPUS))\n",
        "\n",
        "\n",
        "d = train_dataset[0][0][0].shape[0]\n",
        "\n",
        "#First set of Neural Network options, uncomment whichever one you want to run\n",
        "\n",
        "#model = BatchModel(d, hidden_dim)\n",
        "# model = MiNet(d, hidden_dim)\n",
        "# model = miNet(d, hidden_dim)\n",
        "# model = MIL_Pool(d, hidden_dim)\n",
        "# model = TranMil(d, hidden_dim)\n",
        "model = MilGCN(d, hidden_dim)\n",
        "\n",
        "#Multiple versions f the Hopfield network that do not work. You need to uncomment “oneItemOnly” too, as the code doesn’t work with batches at a time.\n",
        "# oneItemOnly = True\n",
        "# hopfield_pooling = HopfieldPooling(input_size=d, hidden_size=128, num_heads = 8)\n",
        "# output_projection = nn.Linear(in_features=hopfield_pooling.output_size, out_features=1)\n",
        "# model = nn.Sequential(hopfield_pooling, output_projection, nn.Flatten(start_dim=0))\n",
        "# hopfield = Hopfield(input_size=d, hidden_size=128, num_heads=8, update_steps_max=3, scaling=0.25, dropout=0.5)\n",
        "# output_projection = nn.Linear(in_features=hopfield.output_size * min_size, out_features=1)\n",
        "# model = nn.Sequential(hopfield, nn.Flatten(), output_projection, nn.Flatten(start_dim=0)).to(device=device)\n",
        "# hopfield = Hopfield(input_size=d, hidden_size=128, num_heads=8, update_steps_max=3, scaling=0.25, dropout=0.5)\n",
        "# hopfield_pooling = HopfieldPooling(input_size=hopfield.output_size, hidden_size=128, num_heads = 8)\n",
        "# output_projection = nn.Linear(in_features=hopfield_pooling.output_size, out_features=1)\n",
        "# model = nn.Sequential(hopfield, hopfield_pooling, output_projection, nn.Flatten(start_dim=0)).to(device=device)\n",
        "\n",
        "opt = torch.optim.AdamW(model.parameters())\n",
        "criteria = nn.BCEWithLogitsLoss()\n",
        "\n",
        "def fixGrad(grad):\n",
        "    grad[torch.isnan(grad)] = 0\n",
        "    return torch.clamp(grad, -5, 5)\n",
        "\n",
        "for p in model.parameters():\n",
        "    p.register_hook(fixGrad)\n",
        "\n",
        "\n",
        "results_per_epoch = {}\n",
        "model = model.to(device)\n",
        "for epoch in tqdm(range(epochs), desc=\"Epoch\"):\n",
        "    opt.zero_grad()\n",
        "    pred_labels = []\n",
        "    true_labels = []\n",
        "    scores = []\n",
        "\n",
        "    model = model.train()\n",
        "    for batch, batch_starts, labels in tqdm(train_loader, leave=False):\n",
        "        batch = torch.tensor(batch, dtype=torch.float32, device=device)\n",
        "        batch_starts = torch.tensor(batch_starts, dtype=torch.int64, device=device)\n",
        "#         labels = torch.tensor(labels, dtype=torch.int64, device=device)\n",
        "        labels = torch.tensor(labels, dtype=torch.float32, device=device).view(-1,1)\n",
        "\n",
        "        if BATCH_SIZE == 1 and oneItemOnly:\n",
        "          output = model(batch.view(1, batch.size(0), -1))\n",
        "          labels = labels.view(1)\n",
        "        else:\n",
        "          output = model(batch, batch_starts)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred_label = (1*(torch.sigmoid(output) > 0.5)).cpu().numpy().tolist()\n",
        "\n",
        "            pred_labels.extend(pred_label)\n",
        "            true_labels.extend(labels.cpu().numpy().tolist())\n",
        "            scores.extend(output.cpu().numpy().tolist())\n",
        "\n",
        "        loss = criteria(output, labels)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "    #Eval Time!\n",
        "    model = model.eval()\n",
        "\n",
        "    pred_labels_eval = []\n",
        "    true_labels_eval = []\n",
        "    scores_eval = []\n",
        "    for batch, batch_starts, labels in tqdm(test_loader, leave=False):\n",
        "        batch = torch.tensor(batch, dtype=torch.float32, device=device)\n",
        "        batch_starts = torch.tensor(batch_starts, dtype=torch.int64, device=device)\n",
        "        labels = torch.tensor(labels, dtype=torch.int64, device=device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if BATCH_SIZE == 1 and oneItemOnly:\n",
        "              output = model(batch.view(1, batch.size(0), -1))\n",
        "              labels = labels.view(1)\n",
        "            else:\n",
        "              output = model(batch, batch_starts)\n",
        "\n",
        "            pred_label = (1*(torch.sigmoid(output) > 0.5)).cpu().numpy().tolist()\n",
        "            scores_eval.extend(output.cpu().numpy().tolist())\n",
        "\n",
        "            pred_labels_eval.extend(pred_label)\n",
        "            true_labels_eval.extend(labels.cpu().numpy().tolist())\n",
        "\n",
        "\n",
        "    scores = np.array(scores)\n",
        "    true_labels = np.array(true_labels).flatten()\n",
        "    pred_labels = np.array(pred_labels)\n",
        "    scores_eval = np.array(scores_eval)\n",
        "    true_labels_eval = np.array(true_labels_eval)\n",
        "    pred_labels_eval = np.array(pred_labels_eval)\n",
        "\n",
        "    scores = scores[true_labels >= 0]\n",
        "    pred_labels = pred_labels[true_labels >= 0]\n",
        "    true_labels = true_labels[true_labels >= 0]\n",
        "    scores_eval = scores_eval[true_labels_eval >= 0]\n",
        "    pred_labels_eval = pred_labels_eval[true_labels_eval >= 0]\n",
        "    true_labels_eval = true_labels_eval[true_labels_eval >= 0]\n",
        "\n",
        "    results_per_epoch[epoch] = {\n",
        "        \"Train: Balanced Accuracy\":balanced_accuracy_score(true_labels, pred_labels),\n",
        "        \"Train: Accuracy\":accuracy_score(true_labels, pred_labels),\n",
        "        \"Train: F1\":f1_score(true_labels, pred_labels),\n",
        "        \"Train: AUC\":roc_auc_score(true_labels, scores),\n",
        "        #####\n",
        "        \"Balanced Accuracy\":balanced_accuracy_score(true_labels_eval, pred_labels_eval),\n",
        "        \"Accuracy\":accuracy_score(true_labels_eval, pred_labels_eval),\n",
        "        \"F1\":f1_score(true_labels_eval, pred_labels_eval),\n",
        "        \"AUC\":roc_auc_score(true_labels_eval, scores_eval),\n",
        "    }\n",
        "#Done training\n",
        "print(results_per_epoch[epochs-1])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}